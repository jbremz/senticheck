{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senticheck\n",
    "\n",
    "_A simple sentiment classifier for the IMDb review dataset_\n",
    "\n",
    "Here I shall go through the steps to train and test a simple LSTM-based sentiment classifier on the [IMDb review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), first using Keras and then with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from utils import *\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "I have written auxillary functions in `utils.py` to load the data into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = makeDF(\"./aclImdb/train\")\n",
    "test_df = makeDF(\"./aclImdb/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              string sentiment\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...       pos\n",
       "1  Homelessness (or Houselessness as George Carli...       pos\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...       pos\n",
       "3  This is easily the most underrated film inn th...       pos\n",
       "4  This is not the typical Mel Brooks film. It wa...       pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              string sentiment\n",
       "0  I went and saw this movie last night after bei...       pos\n",
       "1  Actor turned director Bill Paxton follows up h...       pos\n",
       "2  As a recreational golfer with some knowledge o...       pos\n",
       "3  I saw this film in a sneak preview, and it is ...       pos\n",
       "4  Bill Paxton has taken the true story of the 19...       pos"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make a function to **clean unwanted characters** and numbers from the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(sample):\n",
    "    cleaner = re.compile('<.*?>') # rm characters within <>\n",
    "    sample = re.sub(r'\\d+', '', sample) # rm one or more digits\n",
    "    sample = re.sub(cleaner, '', sample)\n",
    "    sample = re.sub(\"'\", '', sample) # rm single quotes\n",
    "    sample = re.sub(r'\\W+', ' ', sample) # rm non-word characters\n",
    "    sample = sample.replace('_', '') # rm underscores\n",
    "    sample = sample.lower() # make lowercase\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncleaned String:\n",
      " \n",
      "When I first read Armistead Maupins story I was taken in by the human drama displayed by Gabriel No one and those he cares about and loves. That being said, we have now been given the film version of an excellent story and are expected to see past the gloss of Hollywood...<br /><br />Writer Armistead Maupin and director Patrick Stettner have truly succeeded! <br /><br />With just the right amount of restraint Robin Williams captures the fragile essence of Gabriel and lets us see his struggle with\n",
      "\n",
      "Cleaned String:\n",
      " \n",
      "when i first read armistead maupins story i was taken in by the human drama displayed by gabriel no one and those he cares about and loves that being said we have now been given the film version of an excellent story and are expected to see past the gloss of hollywood writer armistead maupin and director patrick stettner have truly succeeded with just the right amount of restraint robin williams captures the fragile essence of gabriel and lets us see his struggle with\n"
     ]
    }
   ],
   "source": [
    "print('Uncleaned String:', '\\n \\n', train_df.loc[10, 'string'][:501],'\\n', sep='')\n",
    "print('Cleaned String:', '\\n \\n', clean_string(train_df.loc[10, 'string'][:501]), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This processing extracts the information we want from the strings (the words and their contexts amongst the other words), whilst also ensuring that words with different formatting are detected as the same _e.g._ `\"When\" = \"when\"`, `\"loves.\" = \"loves\"`.\n",
    "\n",
    "Now we need to process the input data. I will encode each string as an array of integers, only the most popular words (up to `vocab_size` below) will be kept, the others are dropped. I normally use sklearn's `LabelEncoder()` for this purpose, but the `Tokenizer()` in Keras seems to be much easier / more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean samples\n",
    "X_tr = train_df['string'].apply(lambda x: clean_string(x)).values\n",
    "X_te = test_df['string'].apply(lambda x: clean_string(x)).values\n",
    "\n",
    "# Create tokenizer\n",
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, split=' ') \n",
    "tokenizer.fit_on_texts(X_tr)\n",
    "\n",
    "# Integer-encode\n",
    "X_tr = np.array(tokenizer.texts_to_sequences(X_tr))\n",
    "X_te = np.array(tokenizer.texts_to_sequences(X_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the distribution of review lengths in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(lengths,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean review-length: 207 words\n"
     ]
    }
   ],
   "source": [
    "lengths = np.array([[len(x) for x in X_tr],[len(x) for x in X_te]]).flatten()\n",
    "print('Mean review-length: %.0f words' % mean_length.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most fall below 250 words in length.\n",
    "\n",
    "Ideally, we'd take the full reviews for training, however, this would take far too long on my machine. I've chosen to take the first 32 words instead. Any samples _shorter_ than this length are padded with 0 to ensure uniform sequence length so that we can input the data into our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "# maxlen = max([max([len(x) for x in X_tr]),max([len(x) for x in X_te])])\n",
    "maxlen = 32 # only take the first 32 words for the sake of speed\n",
    "X_train = pad_sequences(X_tr, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_te, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall convert the pos/neg column into a binary token 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.get_dummies(train_df['sentiment']).values\n",
    "Y_test = pd.get_dummies(test_df['sentiment']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (25000, 32) \n",
      " X_test shape: (25000, 32)\n",
      "Y_train shape: (25000, 2) \n",
      " Y_test shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape, '\\n', 'X_test shape:', X_test.shape)\n",
    "print('Y_train shape:', Y_train.shape, '\\n', 'Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/test split has already been carried out but we need to now split the test data into the validation/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "I roughly followed the approach taken [here](http://bit.ly/2O4PNEd). The author applies a Recurrent Neural Network with LSTM units to predict the (positive/negative) sentiment of tweets. My problem is essentially the same, except the IMDb dataset I am using contains longer strings for each sample, hence why I have cut down my sequence length above to cut down on computation time.\n",
    "\n",
    "An RNN is an obvious choice of architecture if we are applying deep learning to sentiment analysis of text, as it captures the inherent sequential aspect of language; it has time-dependence. However, traditional RNN's suffer from the problem of vanishing/exploding gradients during training, where the weight matrix becomes unstable.\n",
    "\n",
    "For this reason I'm using LSTM units, which attempt to address this by introducing a number of gates. These, in part, help each unit to determine which previous states will contribute towards the value of the current state. They have been shown to work well in this context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "I added a second hidden LSTM layer to try and capture deeper dependencies between the words. I have also increased the dropout since the model was overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 32, 128)           640000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32, 196)           254800    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 196)               308112    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 1,203,306\n",
      "Trainable params: 1,203,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128 # nodes in the embedding layer\n",
    "lstm_out = 196 # nodes in the lstm layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim,input_length = X_train.shape[1])) # layer for word vectorisation\n",
    "model.add(SpatialDropout1D(0.6)) # prevent overfitting\n",
    "model.add(LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6, return_sequences=True)) # LSTM layer 1\n",
    "model.add(LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6)) # LSTM layer 2\n",
    "model.add(Dense(2,activation='softmax')) # softmax layer\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 12500 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.5966 - acc: 0.6706 - val_loss: 0.4907 - val_acc: 0.7764\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.4947 - acc: 0.7658 - val_loss: 0.4550 - val_acc: 0.7855\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4591 - acc: 0.7894 - val_loss: 0.4516 - val_acc: 0.7879\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.4371 - acc: 0.8018 - val_loss: 0.4512 - val_acc: 0.7869\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4195 - acc: 0.8119 - val_loss: 0.4432 - val_acc: 0.7933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1306c19e8>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.44\n",
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"Score: %.2f\" % (score))\n",
    "print(\"Accuracy: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://blog.paralleldots.com/data-science/breakthrough-research-papers-and-models-for-sentiment-analysis/) article runs through some of the accuracies scored on the IMDb benchmark. Given the size of reviews we used, we haven't performed too poorly.\n",
    "\n",
    "Let's build a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Y_test == 1\n",
    "Y_results = np.tile(np.array(['pos', 'neg']), (mask.shape[0],1))[mask]\n",
    "\n",
    "Y_pred_results = model.predict(X_test)\n",
    "Y_pred_results = np.array(['pos', 'neg'])[np.argmax(Y_pred_results, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(np.array([Y_results, Y_pred_results]).T, columns=['Actual', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Actual Predicted\n",
       "0    pos       pos\n",
       "1    pos       pos\n",
       "2    pos       pos\n",
       "3    neg       neg\n",
       "4    pos       pos"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted  False  True  __all__\n",
      "Actual                         \n",
      "False       4961  1225     6186\n",
      "True        1370  4944     6314\n",
      "__all__     6331  6169    12500\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = ConfusionMatrix(np.where(results['Actual']=='pos',True,False), np.where(results['Predicted']=='pos',True,False))\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where True/False represent positive/negative sentiments respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Accuracy: 0.7830218561925879\n",
      "Negative Accuracy: 0.8019721952796638\n"
     ]
    }
   ],
   "source": [
    "pos = results[results['Actual'] == 'pos']\n",
    "pos_acc = (pos['Actual'] == pos['Predicted']).sum()/pos.shape[0]\n",
    "\n",
    "neg = results[results['Actual'] == 'neg']\n",
    "neg_acc = (neg['Actual'] == neg['Predicted']).sum()/neg.shape[0]\n",
    "\n",
    "print('Positive Accuracy: ', pos_acc, '\\n', 'Negative Accuracy: ', neg_acc, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see a slightly better performance identifying negative reviews, although we'd have to run multiple tests to determine whether this is statistically significant.\n",
    "\n",
    "### Example Errors\n",
    "\n",
    "Let's look at some examples of the strings it predicted wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    6,     7,    13, ..., 12491, 12494, 12498])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_inds = np.argwhere(results['Actual'] != results['Predicted']).flatten()\n",
    "error_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(index, reverse_word_map, results):\n",
    "    err_ind = index\n",
    "    print('Actual: ', results.loc[err_ind, 'Actual'],'\\n','Predicted: ',results.loc[err_ind, 'Predicted'],sep='')\n",
    "    err = [reverse_word_map[x] for x in X_test[err_ind]]\n",
    "    print('Text:')\n",
    "    print(' '.join(err))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "Actual: neg\n",
      "Predicted: pos\n",
      "Text:\n",
      "white cinematography but available in a horrible version adequate music score fitting to suspense by frank the motion picture is r william the usual saga director and in the monsters movies universal\n",
      "\n",
      "\n",
      "2.\n",
      "Actual: pos\n",
      "Predicted: neg\n",
      "Text:\n",
      "but ultimately not recommended and whats with the lack of an dvd i know it was shot in because i saw the pilot episode on one of our channels get with the\n",
      "\n",
      "\n",
      "3.\n",
      "Actual: neg\n",
      "Predicted: pos\n",
      "Text:\n",
      "because i hate it when people give spoilers so i do not want to be one of those people well i guess that is all i have to say about this movie\n",
      "\n",
      "\n",
      "4.\n",
      "Actual: neg\n",
      "Predicted: pos\n",
      "Text:\n",
      "work and an exciting climax its been said that the running time on this one was when it showed in india of robert a master at given a choice for the version\n",
      "\n",
      "\n",
      "5.\n",
      "Actual: neg\n",
      "Predicted: pos\n",
      "Text:\n",
      "typically american the actors performance is ok sandra usually me with her oh my god why me way to behave but this time she seems to have herself id recommend that movie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i+1,'.',sep='')\n",
    "    print_example(error_inds[i], reverse_word_map, results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things to note from these examples:\n",
    "\n",
    "- It is evident that longer samples would be desireable. Some of samples above (e.g. 2) would be difficult to predict even for a human, more context is needed.\n",
    "- The `vocab_size` of 5000 that I used seems a little small. At least to a human, the sentences seem disjointed due to the dropped words. Perhaps increasing the vocab size would increase accuracy (although it was unclear during testing whether changing `vocab_size` did make a noticeable difference). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Examples\n",
    "\n",
    "Let's look at some of the samples that the model predicted _correctly_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_inds = np.argwhere(results['Actual'] == results['Predicted']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "Actual: pos\n",
      "Predicted: pos\n",
      "Text:\n",
      "plain common sense would ever do that i guess the actor figured that his level the longer he holds the gun that way this movie is such an insult to common sense\n",
      "\n",
      "\n",
      "2.\n",
      "Actual: pos\n",
      "Predicted: pos\n",
      "Text:\n",
      "for me if you get a chance to watch the mstk version you wont be disappointed by it self not so much but i can think of worse methods of torture anyone\n",
      "\n",
      "\n",
      "3.\n",
      "Actual: pos\n",
      "Predicted: pos\n",
      "Text:\n",
      "around is yet another poorly made film about spirits attempting to warn people away from a house if theres any message that the delivers its dont waste your time on this movie\n",
      "\n",
      "\n",
      "4.\n",
      "Actual: neg\n",
      "Predicted: neg\n",
      "Text:\n",
      "also made in mexico several years later it deals with a similar subject matter the of the ego in the face of only it does so in a funny and totally style\n",
      "\n",
      "\n",
      "5.\n",
      "Actual: pos\n",
      "Predicted: pos\n",
      "Text:\n",
      "interesting unfortunately the dialogue wasnt really there for them im giving it a for their performances but nothing else here is worth mentioning hopefully these actors will get roles worthy of them\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i+1,'.',sep='')\n",
    "    print_example(corr_inds[i], reverse_word_map, results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, some of these samples I would have predicted _incorrectly_ (e.g. 1, 3). It's difficult to tell if the model is just 'lucky' in these cases or whether it actually has a deeper statistical understanding of the reviews. Otherwise, the results are as one might expect (e.g. 2, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "Thoughts for future improvements:\n",
    "\n",
    "- Train the network on a larger sequence of each review\n",
    "- Use a larger vocab\n",
    "- Perhaps try using a CNN - have been shown to work well on movie reviews [here](https://www.aclweb.org/anthology/D14-1181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try building a similar model but using pure Tensorflow, this time with only one layer of LSTM cells. I am following the (modified) approach for implementing LSTMs in Tensorflow from [here](https://bit.ly/2DOGvYh) and [here](https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We first need to create a class which will let us store our data as an object. It has simple methods to shuffle the data and to return batches iteratively, these will be used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.size = data[0].shape[0]\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        s = np.arange(self.data[0].shape[0])\n",
    "        np.random.shuffle(s)\n",
    "        self.data = [self.data[0][s],self.data[1][s]]\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        X_batch = self.data[0][self.cursor:self.cursor+n]\n",
    "        Y_batch = self.data[1][self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return X_batch, Y_batch, np.array(n*[X_batch.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Y from one-hot encoded to integer encoded\n",
    "train = [X_train, np.argmax(Y_train, axis=1)] \n",
    "val = [X_val, np.argmax(Y_val, axis=1)]\n",
    "test = [X_test, np.argmax(Y_test, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def build_graph(\n",
    "    vocab_size = vocab_size,\n",
    "    state_size = 64,\n",
    "    batch_size = 32,\n",
    "    num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, shape=[batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    y = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    keep_prob = tf.placeholder(tf.float32,[])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', shape=[vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # LSTM\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(state_size)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=keep_prob)\n",
    "    rnn_outputs, _ = tf.nn.dynamic_rnn(lstmCell, rnn_inputs, dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob, # should change this to keep_prob rather than dropout, it's confusing\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(graph, batch_size = 32, num_epochs = 5, iterator = SimpleDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        va = iterator(val)\n",
    "        g = graph\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #val set accuracy\n",
    "                te_epoch = va.epochs\n",
    "                while va.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = va.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.6}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - acc:\", tr_losses[-1], \"- val_acc:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - acc: 0.654891304347826 - val_acc: 0.7559942455242967\n",
      "Accuracy after epoch 2  - acc: 0.7873319462227913 - val_acc: 0.7741185897435897\n",
      "Accuracy after epoch 3  - acc: 0.814940781049936 - val_acc: 0.7791666666666667\n",
      "Accuracy after epoch 4  - acc: 0.8315861075544174 - val_acc: 0.7808493589743589\n",
      "Accuracy after epoch 5  - acc: 0.8421494878361075 - val_acc: 0.7738782051282052\n"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't have time to implement a proper evaluation of this model's accuracy, but we can see that on the validation dataset the model performance is almost on par with my Keras model with two LSTM layers. The limiting factor for further accuracy improvements is, as before, likely to be the short sequence-length.\n",
    "\n",
    "There may be some overfitting occurring in the final epoch as the validation dataset accuracy reduces, this persisted despite me tuning the `keep_prob` (`keep_prob = 1 - dropout_prob`). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
