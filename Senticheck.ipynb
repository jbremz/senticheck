{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senticheck\n",
    "\n",
    "_A simple sentiment classifier for the IMDb review dataset_\n",
    "\n",
    "Here I go through the steps to train and test a simple LSTM-based sentiment classifier on the IMDb review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from utils import *\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from pandas_ml import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "I have written auxillary functions in `utils.py` to load the data into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = makeDF(\"./aclImdb/train\")\n",
    "test_df = makeDF(\"./aclImdb/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              string sentiment\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...       pos\n",
       "1  Homelessness (or Houselessness as George Carli...       pos\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...       pos\n",
       "3  This is easily the most underrated film inn th...       pos\n",
       "4  This is not the typical Mel Brooks film. It wa...       pos"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              string sentiment\n",
       "0  I went and saw this movie last night after bei...       pos\n",
       "1  Actor turned director Bill Paxton follows up h...       pos\n",
       "2  As a recreational golfer with some knowledge o...       pos\n",
       "3  I saw this film in a sneak preview, and it is ...       pos\n",
       "4  Bill Paxton has taken the true story of the 19...       pos"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make a function to **clean unwanted characters** and numbers from the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(sample):\n",
    "    cleaner = re.compile('<.*?>') # rm characters within <>\n",
    "    sample = re.sub(r'\\d+', '', sample) # rm one or more digits\n",
    "    sample = re.sub(cleaner, '', sample)\n",
    "    sample = re.sub(\"'\", '', sample) # rm single quotes\n",
    "    sample = re.sub(r'\\W+', ' ', sample) # rm non-word characters\n",
    "    sample = sample.replace('_', '') # rm underscores\n",
    "    sample = sample.lower() # make lowercase\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncleaned String:\n",
      " \n",
      "When I first read Armistead Maupins story I was taken in by the human drama displayed by Gabriel No one and those he cares about and loves. That being said, we have now been given the film version of an excellent story and are expected to see past the gloss of Hollywood...<br /><br />Writer Armistead Maupin and director Patrick Stettner have truly succeeded! <br /><br />With just the right amount of restraint Robin Williams captures the fragile essence of Gabriel and lets us see his struggle with\n",
      "\n",
      "Cleaned String:\n",
      " \n",
      "when i first read armistead maupins story i was taken in by the human drama displayed by gabriel no one and those he cares about and loves that being said we have now been given the film version of an excellent story and are expected to see past the gloss of hollywood writer armistead maupin and director patrick stettner have truly succeeded with just the right amount of restraint robin williams captures the fragile essence of gabriel and lets us see his struggle with\n"
     ]
    }
   ],
   "source": [
    "print('Uncleaned String:', '\\n \\n', train_df.loc[10, 'string'][:501],'\\n', sep='')\n",
    "print('Cleaned String:', '\\n \\n', clean_string(train_df.loc[10, 'string'][:501]), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This processing extracts the information we want from the strings (the words and their contexts amongst the other words), whilst also ensuring that words with different formatting are detected as the same _e.g._ `When = when`, `loves. = loves`.\n",
    "\n",
    "Now we need to process the input data. I will encode each string as an array of integers, only the most popular words (up to `vocab_size` below) will be kept, the others are dropped. I normally use sklearn's `LabelEncoder()` for this purpose, but the `Tokenizer()` in Keras seems to be much easier / more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean samples\n",
    "X_tr = train_df['string'].apply(lambda x: clean_string(x)).values\n",
    "X_te = test_df['string'].apply(lambda x: clean_string(x)).values\n",
    "\n",
    "# Create tokenizer\n",
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, split=' ') \n",
    "tokenizer.fit_on_texts(X_tr)\n",
    "\n",
    "# Integer-encode\n",
    "X_tr = np.array(tokenizer.texts_to_sequences(X_tr))\n",
    "X_te = np.array(tokenizer.texts_to_sequences(X_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd take the full reviews for training, however, this would take far too long on my machine. I've chosen to take the first 32 words instead. Any samples _shorter_ than this length are padded with 0 to ensure uniform sequence length so that we can input the data into our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "# maxlen = max([max([len(x) for x in X_tr]),max([len(x) for x in X_te])])\n",
    "maxlen = 32 # only take the first 32 words for the sake of speed\n",
    "X_train = pad_sequences(X_tr, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_te, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall convert the pos/neg column into a binary token 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.get_dummies(train_df['sentiment']).values\n",
    "Y_test = pd.get_dummies(test_df['sentiment']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (25000, 32) \n",
      " X_test shape: (25000, 32)\n",
      "Y_train shape: (25000, 2) \n",
      " Y_test shape: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape, '\\n', 'X_test shape:', X_test.shape)\n",
    "print('Y_train shape:', Y_train.shape, '\\n', 'Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/test split has already been carried out but we need to now split the test data into the validation/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "I roughly followed the approach taken [here](http://bit.ly/2O4PNEd). The author applies a Recurrent Neural Network with LSTM units to predict the (positive/negative) sentiment of tweets. My problem is essentially the same, except the IMDb dataset I am using contains longer strings for each sample, hence why I have cut down my sequence length above.\n",
    "\n",
    "An RNN is an obvious choice of architecture if we are applying deep learning to sentiment analysis of text, as it captures the inherent sequential aspect of language - there is time-dependence. However, traditional RNN's suffer from the problem of vanishing/exploding gradients during training, where the weight matrix becomes unstable.\n",
    "\n",
    "For this reason I'm using LSTM units, which attempt to address this by introducing forget gates to select which previous states contribute to the state of the current cell. They have been shown to work well in this context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "I added a second hidden LSTM layer to try and capture deeper dependencies between the words. I have also increased the dropout since the model was overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 32, 128)           640000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32, 196)           254800    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 196)               308112    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 1,203,306\n",
      "Trainable params: 1,203,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128 # nodes in the embedding layer\n",
    "lstm_out = 196 # nodes in the lstm layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim,input_length = X_train.shape[1])) # layer for word vectorisation\n",
    "model.add(SpatialDropout1D(0.6)) # prevent overfitting\n",
    "model.add(LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6, return_sequences=True)) # LSTM layer 1\n",
    "model.add(LSTM(lstm_out, dropout=0.6, recurrent_dropout=0.6)) # LSTM layer 2\n",
    "model.add(Dense(2,activation='softmax')) # softmax layer\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 12500 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.5966 - acc: 0.6706 - val_loss: 0.4907 - val_acc: 0.7764\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.4947 - acc: 0.7658 - val_loss: 0.4550 - val_acc: 0.7855\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4591 - acc: 0.7894 - val_loss: 0.4516 - val_acc: 0.7879\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.4371 - acc: 0.8018 - val_loss: 0.4512 - val_acc: 0.7869\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4195 - acc: 0.8119 - val_loss: 0.4432 - val_acc: 0.7933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1306c19e8>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.44\n",
      "Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"Score: %.2f\" % (score))\n",
    "print(\"Accuracy: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://blog.paralleldots.com/data-science/breakthrough-research-papers-and-models-for-sentiment-analysis/) article runs through some of the accuracies scored on the IMDb benchmark. Given the size of reviews we used, we haven't performed too poorly.\n",
    "\n",
    "Let's build a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Y_test == 1\n",
    "Y_results = np.tile(np.array(['pos', 'neg']), (mask.shape[0],1))[mask]\n",
    "\n",
    "Y_pred_results = model.predict(X_test)\n",
    "Y_pred_results = np.array(['pos', 'neg'])[np.argmax(Y_pred_results, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(np.array([Y_results, Y_pred_results]).T, columns=['Actual', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Actual Predicted\n",
       "0    pos       pos\n",
       "1    pos       pos\n",
       "2    pos       pos\n",
       "3    neg       neg\n",
       "4    pos       pos"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "Predicted  False  True  __all__\n",
      "Actual                         \n",
      "False       4961  1225     6186\n",
      "True        1370  4944     6314\n",
      "__all__     6331  6169    12500\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = ConfusionMatrix(np.where(results['Actual']=='pos',True,False), np.where(results['Predicted']=='pos',True,False))\n",
    "print(\"Confusion matrix:\\n%s\" % confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where True/False represent positive/negative sentiments respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Accuracy: 0.7830218561925879\n",
      "Negative Accuracy: 0.8019721952796638\n"
     ]
    }
   ],
   "source": [
    "pos = results[results['Actual'] == 'pos']\n",
    "pos_acc = (pos['Actual'] == pos['Predicted']).sum()/pos.shape[0]\n",
    "\n",
    "neg = results[results['Actual'] == 'neg']\n",
    "neg_acc = (neg['Actual'] == neg['Predicted']).sum()/neg.shape[0]\n",
    "\n",
    "print('Positive Accuracy: ', pos_acc, '\\n', 'Negative Accuracy: ', neg_acc, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see a slightly better performance identifying negative reviews, although we'd have to run multiple tests to determine whether this is statistically significant.\n",
    "\n",
    "### Example Errors\n",
    "\n",
    "Let's look at some examples of the strings it predicted wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    6,     7,    13, ..., 12491, 12494, 12498])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_inds = np.argwhere(results['Actual'] != results['Predicted']).flatten()\n",
    "error_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(index, reverse_word_map, results):\n",
    "    err_ind = index\n",
    "    print('Actual: ', results.loc[err_ind, 'Actual'],'\\n','Predicted: ',results.loc[err_ind, 'Predicted'],sep='')\n",
    "    err = [reverse_word_map[x] for x in X_test[err_ind]]\n",
    "    print('Text:')\n",
    "    print(' '.join(err))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "Actual: neg\n",
      "Predicted: pos\n",
      "Text:\n",
      "white cinematography but available in a horrible version adequate music score fitting to suspense by frank the motion picture is r william the usual saga director and in the monsters movies universal\n",
      "\n",
      "\n",
      "2.\n",
      "Actual: pos\n",
      "Predicted: neg\n",
      "Text:\n",
      "but ultimately not recommended and whats with the lack of an dvd i know it was shot in because i saw the pilot episode on one of our channels get with the\n",
      "\n",
      "\n",
      "3.\n",
      "Actual: neg\n",
      "Predicted: pos\n",
      "Text:\n",
      "because i hate it when people give spoilers so i do not want to be one of those people well i guess that is all i have to say about this movie\n",
      "\n",
      "\n",
      "4.\n",
      "Actual: neg\n",
      "Predicted: pos\n",
      "Text:\n",
      "work and an exciting climax its been said that the running time on this one was when it showed in india of robert a master at given a choice for the version\n",
      "\n",
      "\n",
      "5.\n",
      "Actual: neg\n",
      "Predicted: pos\n",
      "Text:\n",
      "typically american the actors performance is ok sandra usually me with her oh my god why me way to behave but this time she seems to have herself id recommend that movie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i+1,'.',sep='')\n",
    "    print_example(error_inds[i], reverse_word_map, results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things to note from these examples:\n",
    "\n",
    "- It is evident that longer samples would be desireable. Some of samples above (e.g. 2) would be difficult to predict even for a human, more context is needed.\n",
    "- The `vocab_size` of 5000 that I used seems a little small. At least to a human, the sentences seem disjointed due to the dropped words. Perhaps increasing the vocab size would increase accuracy (although it was unclear during testing whether changing `vocab_size` did make a noticeable difference). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct Examples\n",
    "\n",
    "Let's look at some of the samples that the model predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_inds = np.argwhere(results['Actual'] == results['Predicted']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "Actual: pos\n",
      "Predicted: pos\n",
      "Text:\n",
      "plain common sense would ever do that i guess the actor figured that his level the longer he holds the gun that way this movie is such an insult to common sense\n",
      "\n",
      "\n",
      "2.\n",
      "Actual: pos\n",
      "Predicted: pos\n",
      "Text:\n",
      "for me if you get a chance to watch the mstk version you wont be disappointed by it self not so much but i can think of worse methods of torture anyone\n",
      "\n",
      "\n",
      "3.\n",
      "Actual: pos\n",
      "Predicted: pos\n",
      "Text:\n",
      "around is yet another poorly made film about spirits attempting to warn people away from a house if theres any message that the delivers its dont waste your time on this movie\n",
      "\n",
      "\n",
      "4.\n",
      "Actual: neg\n",
      "Predicted: neg\n",
      "Text:\n",
      "also made in mexico several years later it deals with a similar subject matter the of the ego in the face of only it does so in a funny and totally style\n",
      "\n",
      "\n",
      "5.\n",
      "Actual: pos\n",
      "Predicted: pos\n",
      "Text:\n",
      "interesting unfortunately the dialogue wasnt really there for them im giving it a for their performances but nothing else here is worth mentioning hopefully these actors will get roles worthy of them\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i+1,'.',sep='')\n",
    "    print_example(corr_inds[i], reverse_word_map, results)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, some of these samples I would have predicted _incorrectly_ (e.g. 1, 3). It's difficult to tell if the model is just 'lucky' in these cases or whether it actually has a deeper statistical understanding of the reviews. Otherwise, the results are as one might expect (e.g. 2, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "Thoughts for future improvements:\n",
    "\n",
    "- Train the network on the full length of each review\n",
    "- Use a larger vocab\n",
    "- Perhaps try using a CNN - have been shown to work well on movie reviews [here](https://www.aclweb.org/anthology/D14-1181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try running a similar model but with lower-level Tensorflow. Roughly following the code from [here](https://bit.ly/2DOGvYh) and [here](https://www.oreilly.com/learning/perform-sentiment-analysis-with-lstms-using-tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.size = data[0].shape[0]\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        s = np.arange(self.data[0].shape[0])\n",
    "        np.random.shuffle(s)\n",
    "        self.data = [self.data[0][s],self.data[1][s]]\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        X_batch = self.data[0][self.cursor:self.cursor+n]\n",
    "        Y_batch = self.data[1][self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        return X_batch, Y_batch, np.array(n*[X_batch.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Y from one-hot encoded to integer encoded\n",
    "train = [X_train, np.argmax(Y_train, axis=1)] \n",
    "val = [X_val, np.argmax(Y_val, axis=1)]\n",
    "test = [X_test, np.argmax(Y_test, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def build_graph(\n",
    "    vocab_size = vocab_size,\n",
    "    state_size = 64,\n",
    "    batch_size = 32,\n",
    "    num_classes = 2):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.int32, shape=[batch_size, None]) # [batch_size, num_steps]\n",
    "    seqlen = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    y = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    keep_prob = tf.placeholder(tf.float32,[])\n",
    "\n",
    "    # Embedding layer\n",
    "    embeddings = tf.get_variable('embedding_matrix', shape=[vocab_size, state_size])\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    # LSTM\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(state_size)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "    rnn_outputs, _ = tf.nn.dynamic_rnn(lstmCell, rnn_inputs, dtype=tf.float32)\n",
    "\n",
    "    # Add dropout, as the model otherwise quickly overfits\n",
    "    rnn_outputs = tf.nn.dropout(rnn_outputs, keep_prob)\n",
    "\n",
    "    idx = tf.range(batch_size)*tf.shape(rnn_outputs)[1] + (seqlen - 1)\n",
    "    last_rnn_output = tf.gather(tf.reshape(rnn_outputs, [-1, state_size]), idx)\n",
    "\n",
    "    # Softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = tf.matmul(last_rnn_output, W) + b\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    return {\n",
    "        'x': x,\n",
    "        'seqlen': seqlen,\n",
    "        'y': y,\n",
    "        'dropout': keep_prob,\n",
    "        'loss': loss,\n",
    "        'ts': train_step,\n",
    "        'preds': preds,\n",
    "        'accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(graph, batch_size = 32, num_epochs = 10, iterator = SimpleDataIterator):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr = iterator(train)\n",
    "        te = iterator(val)\n",
    "        g = graph\n",
    "\n",
    "        step, accuracy = 0, 0\n",
    "        tr_losses, te_losses = [], []\n",
    "        current_epoch = 0\n",
    "        while current_epoch < num_epochs:\n",
    "            step += 1\n",
    "            batch = tr.next_batch(batch_size)\n",
    "            feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.2}\n",
    "            accuracy_, _ = sess.run([g['accuracy'], g['ts']], feed_dict=feed)\n",
    "            accuracy += accuracy_\n",
    "\n",
    "            if tr.epochs > current_epoch:\n",
    "                current_epoch += 1\n",
    "                tr_losses.append(accuracy / step)\n",
    "                step, accuracy = 0, 0\n",
    "\n",
    "                #eval test set\n",
    "                te_epoch = te.epochs\n",
    "                while te.epochs == te_epoch:\n",
    "                    step += 1\n",
    "                    batch = te.next_batch(batch_size)\n",
    "                    feed = {g['x']: batch[0], g['y']: batch[1], g['seqlen']: batch[2], g['dropout']: 0.2}\n",
    "                    accuracy_ = sess.run([g['accuracy']], feed_dict=feed)[0]\n",
    "                    accuracy += accuracy_\n",
    "\n",
    "                te_losses.append(accuracy / step)\n",
    "                step, accuracy = 0,0\n",
    "                print(\"Accuracy after epoch\", current_epoch, \" - tr:\", tr_losses[-1], \"- te:\", te_losses[-1])\n",
    "\n",
    "    return tr_losses, te_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JBremner/Envs/senticheck/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after epoch 1  - tr: 0.5847186700767263 - te: 0.715153452685422\n",
      "Accuracy after epoch 2  - tr: 0.7566021126760564 - te: 0.7697115384615385\n",
      "Accuracy after epoch 3  - tr: 0.8029769526248399 - te: 0.7755608974358974\n",
      "Accuracy after epoch 4  - tr: 0.8200224071702945 - te: 0.7765224358974359\n",
      "Accuracy after epoch 5  - tr: 0.8307058258642765 - te: 0.7765224358974359\n",
      "Accuracy after epoch 6  - tr: 0.8371478873239436 - te: 0.7736378205128205\n",
      "Accuracy after epoch 7  - tr: 0.8446302816901409 - te: 0.7744391025641025\n",
      "Accuracy after epoch 8  - tr: 0.8487916133162612 - te: 0.7720352564102564\n",
      "Accuracy after epoch 9  - tr: 0.853913252240717 - te: 0.7692307692307693\n",
      "Accuracy after epoch 10  - tr: 0.8617157490396927 - te: 0.7694711538461538\n"
     ]
    }
   ],
   "source": [
    "g = build_graph()\n",
    "tr_losses, te_losses = train_graph(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
